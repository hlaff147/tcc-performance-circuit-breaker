\documentclass{svproc}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{float}
\usepackage{booktabs}

\def\UrlFont{\rmfamily}

% Definição do ambiente Resumo (português) - para documentos acadêmicos brasileiros
\newenvironment{resumo}{%
\small
\begin{center}%
{\bfseries Resumo\vspace{-.5em}\vspace{\z}}%
\end{center}%
\quotation}%
{\endquotation}

% Definição do comando palavraschave
\providecommand{\palavraschave}[1]{\par\addvspace\baselineskip
\noindent\textbf{Palavras-chave:} #1}

\begin{document}
\mainmatter
\title{Análise de Desempenho e Resiliência em Microsserviços Síncronos: Um Estudo Experimental do Padrão Circuit Breaker}
\titlerunning{Análise de Desempenho e Resiliência em Microsserviços}

\author{Humberto L. A. Fonseca Filho}
\authorrunning{H. L. A. Fonseca Filho}
\tocauthor{Fonseca Filho, H. L. A.}

\institute{Centro de Informática, Universidade Federal de Pernambuco (UFPE) \\
\email{hlaff@cin.ufpe.br}}

\maketitle

\begin{resumo}
Arquiteturas de microsserviços tornaram-se o padrão para construção de sistemas distribuídos escaláveis, porém a comunicação síncrona entre serviços introduz vulnerabilidades críticas: quando uma dependência downstream degrada ou falha, falhas em cascata podem se propagar por todo o sistema, levando à exaustão do pool de threads e indisponibilidade total do serviço. Embora o padrão Circuit Breaker seja amplamente recomendado como estratégia de mitigação, evidências empíricas quantificando sua eficácia em cenários realistas de falha permanecem escassas na literatura. Este trabalho preenche essa lacuna através de um estudo experimental controlado comparando uma arquitetura baseline contra uma protegida por Circuit Breaker (implementado com Resilience4j) em quatro cenários de estresse: falha catastrófica, degradação gradual, rajadas intermitentes e indisponibilidade extrema. Utilizando microsserviços orquestrados via Docker e testes de carga com k6 totalizando mais de 380.000 requisições por versão, foram medidos vazão, latência (p95) e taxas de sucesso. Os resultados demonstram que o Circuit Breaker proporciona ganhos substanciais de resiliência: no cenário de indisponibilidade extrema (75\% de downtime), as taxas de sucesso melhoraram de 10,14\% para 97,08\% (+86,94 pontos percentuais), representando uma redução de 96,77\% nas falhas. A análise estatística (teste de Mann-Whitney U, p $<$ 0,001; Cliff's Delta = 0,0155) confirma que esses benefícios vêm com overhead de desempenho negligível durante operação normal. Este estudo fornece evidência empírica reprodutível e um framework experimental reutilizável para avaliação de padrões de tolerância a falhas em arquiteturas de microsserviços síncronos.
\palavraschave{Microsserviços, Circuit Breaker, Resiliência, Tolerância a Falhas, Sistemas Distribuídos, Engenharia de Desempenho, Resilience4j}
\end{resumo}

\begin{abstract}
Microservices architectures have become the standard for building scalable distributed systems, yet synchronous inter-service communication introduces critical vulnerabilities: when a downstream dependency degrades or fails, cascading failures can propagate throughout the entire system, leading to thread pool exhaustion and complete service unavailability. While the Circuit Breaker pattern is widely recommended as a mitigation strategy, empirical evidence quantifying its effectiveness under realistic failure scenarios remains scarce in the literature. This work addresses this gap by conducting a controlled experimental study comparing a baseline architecture against one protected by Circuit Breaker (implemented with Resilience4j) across four stress scenarios: catastrophic failure, gradual degradation, intermittent bursts, and extreme unavailability. Using Docker-orchestrated microservices and k6 load testing with over 380,000 requests per version, we measured throughput, latency (p95), and success rates. Results demonstrate that the Circuit Breaker delivers substantial resilience gains: in the extreme unavailability scenario (75\% downtime), success rates improved from 10.14\% to 97.08\% (+86.94 percentage points), representing a 96.77\% reduction in failures. Statistical analysis (Mann-Whitney U test, p $<$ 0.001; Cliff's Delta = 0.0155) confirms that these benefits come with negligible performance overhead during normal operation. This study provides reproducible empirical evidence and a reusable experimental framework for evaluating fault tolerance patterns in synchronous microservices architectures.
\keywords{Microservices, Circuit Breaker, Resilience, Fault Tolerance, Distributed Systems, Performance Engineering, Resilience4j}
\end{abstract}

\section{Contextualização}
A era dos microsserviços consolidou-se no cenário corporativo. A adoção de arquiteturas de microsserviços tornou-se onipresente em organizações que precisam construir plataformas digitais de grande escala, disponibilidade contínua e ciclos de evolução acelerados. Ecossistemas de comércio eletrônico e, em especial, sistemas de processamento de pagamentos, são exemplos emblemáticos desse movimento, pois demandam flexibilidade para incorporar novos meios de pagamento, tolerância a falhas e capacidade de adaptação rápida a volumes variáveis de transações. O particionamento de funcionalidades em serviços independentes facilita o desenvolvimento paralelo, a escalabilidade seletiva e a implantação contínua. Contudo, essa independência lógica é sustentada por interações em tempo real entre serviços, frequentemente estabelecidas por APIs REST e clientes declarativos como o Spring Cloud OpenFeign. A comunicação síncrona simplifica a implementação e a observabilidade, mas introduz um forte acoplamento temporal: o serviço consumidor permanece bloqueado até que a resposta do serviço dependente seja recebida ou um timeout seja atingido. Esse padrão, quando aplicado a cadeias críticas como autorizações financeiras, amplifica a fragilidade do sistema diante de latências elevadas ou indisponibilidade intermitente na infraestrutura externa.

\section{Definição do Problema}

O risco inerente à comunicação síncrona constitui o cerne desta investigação. O problema investigado neste trabalho — \textbf{falhas em cascata causadas por comunicação síncrona entre microsserviços} — é transversal a diversos domínios da engenharia de software: e-commerce, logística, saúde, fintechs, streaming, IoT, entre outros. Qualquer sistema distribuído que dependa de chamadas HTTP síncronas está sujeito aos riscos aqui analisados.

Este trabalho adota o \textbf{domínio de pagamentos on-line} como contexto experimental. A escolha foi motivada pela relevância do domínio na área de sistemas financeiros, onde a resiliência é requisito crítico e falhas têm impacto direto na receita e experiência do usuário. Contudo, os resultados e conclusões deste estudo são \textbf{generalizáveis} para qualquer arquitetura de microsserviços com dependências síncronas.

\textbf{Cenário Hipotético Modelado:} Um \texttt{servico-pagamento} orquestra a jornada de checkout e depende, de forma síncrona, de um \texttt{servico-adquirente} responsável por encaminhar transações a gateways externos (Cielo, Rede, etc.). O risco emerge quando essa dependência apresenta degradação: se o \texttt{servico-adquirente} experimenta alta latência, o \texttt{servico-pagamento} aguarda até o timeout, mantendo threads bloqueadas. Com volume crescente de requisições, o pool de threads se esgota (\textbf{thread pool starvation}), provocando \textbf{falha em cascata} que interrompe todo o checkout.

\section{Solução Proposta}
Os padrões de resiliência e o Circuit Breaker emergem como resposta aos desafios descritos. Padrões de resiliência, tradicionalmente agrupados sob o guarda-chuva de \textit{fault tolerance}, surgem como contramedidas a essa fragilidade. Entre eles, o padrão Circuit Breaker (CB) destaca-se como um mecanismo sofisticado para conter falhas em cascata. O CB monitora as chamadas ao serviço dependente e opera segundo uma máquina de estados composta por três modos: \textbf{Fechado}, \textbf{Aberto} e \textbf{Semiaberto}. No estado Fechado, o serviço consumidor executa chamadas normalmente e o CB coleta métricas de sucesso e falha. Ao detectar uma taxa de falhas ou tempo de resposta acima de limites configurados, o circuito é \textbf{Aberto}, interrompendo novas tentativas de chamadas e falhando rapidamente (\textit{fail-fast}). Esse comportamento protege o serviço consumidor da exaustão de recursos e evita contribuir para a sobrecarga do serviço dependente. Após um período de resfriamento, o CB entra no estado \textbf{Semiaberto}, permitindo um número controlado de chamadas de teste. Se elas forem bem-sucedidas, o circuito retorna ao estado Fechado; caso contrário, reabre. Além disso, é comum empregar mecanismos de \textit{fallback} para entregar respostas degradadas, mas ainda úteis, enquanto o serviço dependente se recupera. Dessa forma, o Circuit Breaker harmoniza a necessidade de disponibilidade do consumidor com a estabilidade do ecossistema.

\section{Fundamentação Teórica}

\subsection{Arquitetura de Microsserviços}
A arquitetura de microsserviços representa uma abordagem de desenvolvimento de software que estrutura uma aplicação como um conjunto de serviços pequenos, autônomos e fracamente acoplados \cite{newman2021}. Cada serviço é responsável por uma capacidade de negócio específica, pode ser desenvolvido, implantado e escalado de forma independente, e se comunica com outros serviços através de interfaces bem definidas, geralmente APIs REST ou mensageria assíncrona \cite{fowler2014}.

Os principais benefícios desta arquitetura incluem: (i) \textbf{escalabilidade seletiva}, permitindo escalar apenas os serviços sob maior demanda; (ii) \textbf{autonomia de equipes}, possibilitando que times diferentes desenvolvam e implantem serviços independentemente; (iii) \textbf{resiliência a falhas}, onde a falha de um serviço não necessariamente compromete todo o sistema; e (iv) \textbf{flexibilidade tecnológica}, permitindo que cada serviço utilize a tecnologia mais adequada ao seu propósito \cite{richardson2018}.

Contudo, essa arquitetura introduz desafios significativos de natureza distribuída. A comunicação entre serviços através da rede é inerentemente não confiável, sujeita a latências variáveis, timeouts e falhas parciais. Em sistemas monolíticos, chamadas de função são executadas em memória com latência desprezível; em microsserviços, cada chamada atravessa a rede e pode falhar de múltiplas formas \cite{nygard2018}.

\subsection{Comunicação Síncrona e seus Riscos}
A comunicação síncrona, tipicamente implementada via HTTP/REST, é caracterizada pelo bloqueio do serviço consumidor enquanto aguarda a resposta do serviço provedor. Embora seja simples de implementar e depurar, este modelo introduz um \textbf{acoplamento temporal} entre os serviços: se o provedor estiver lento ou indisponível, o consumidor também será afetado \cite{burns2018}.

Os principais riscos associados à comunicação síncrona incluem:

\begin{itemize}
    \item \textbf{Thread Pool Starvation:} Quando múltiplas requisições aguardam respostas lentas, as threads do servidor ficam bloqueadas, esgotando o pool disponível e impedindo o processamento de novas requisições.
    \item \textbf{Falhas em Cascata:} Uma dependência lenta ou indisponível pode propagar sua condição para todos os serviços que dela dependem, amplificando o impacto de uma falha localizada para todo o ecossistema.
    \item \textbf{Efeito Dominó:} Em cadeias de dependências (A → B → C), a falha de C pode derrubar B e, consequentemente, A, mesmo que estes estejam funcionando corretamente.
\end{itemize}

\subsection{Padrões de Tolerância a Falhas}
Para mitigar os riscos da comunicação distribuída, diversos padrões de tolerância a falhas foram propostos pela comunidade de engenharia de software \cite{nygard2018, richardson2018}:

\textbf{Timeout:} Define um limite máximo de espera por uma resposta. Embora essencial, não é suficiente isoladamente, pois o serviço ainda consome recursos durante a espera.

\textbf{Retry:} Reenvia automaticamente requisições que falharam, útil para falhas transitórias. Deve ser implementado com backoff exponencial para evitar sobrecarga do serviço dependente.

\textbf{Bulkhead:} Isola recursos (threads, conexões) por dependência, impedindo que a falha de uma dependência consuma todos os recursos do serviço.

\textbf{Rate Limiter:} Controla a taxa de requisições enviadas ou recebidas, protegendo serviços de sobrecarga.

\textbf{Circuit Breaker:} Interrompe temporariamente chamadas a uma dependência que apresenta falhas recorrentes, permitindo recuperação e evitando desperdício de recursos.

\subsection{O Padrão Circuit Breaker em Detalhe}
O padrão Circuit Breaker, popularizado por Michael Nygard em seu livro ``Release It!'' \cite{nygard2018} e documentado por Martin Fowler \cite{fowler2014cb}, opera como um disjuntor elétrico: quando detecta condições anormais, ``abre'' para interromper o fluxo e proteger o sistema.

A implementação típica do Circuit Breaker utiliza uma máquina de estados com três estados:

\begin{enumerate}
    \item \textbf{Fechado (Closed):} Estado normal de operação. Todas as requisições são encaminhadas à dependência. O CB monitora continuamente métricas como taxa de falha e tempo de resposta.
    \item \textbf{Aberto (Open):} Ativado quando métricas excedem limiares configurados (ex: >50\% de falhas). Requisições são imediatamente rejeitadas ou redirecionadas a um fallback, sem tentar contactar a dependência. Após um período de espera (\textit{wait duration}), transiciona para Semiaberto.
    \item \textbf{Semiaberto (Half-Open):} Estado de teste. Um número limitado de requisições é permitido para verificar se a dependência se recuperou. Se bem-sucedidas, retorna ao estado Fechado; caso contrário, volta ao estado Aberto.
\end{enumerate}

\subsection{Implementações de Circuit Breaker: Hystrix vs Resilience4j}
O Netflix Hystrix foi pioneiro na implementação do padrão Circuit Breaker para a JVM, sendo amplamente adotado na indústria \cite{hystrix, netflix2016}. Contudo, em 2018, o projeto entrou em modo de manutenção, e o Resilience4j emergiu como seu sucessor recomendado \cite{resilience4j}.

O Resilience4j apresenta vantagens significativas sobre o Hystrix:

\begin{itemize}
    \item \textbf{Design modular:} Cada padrão (Circuit Breaker, Retry, Bulkhead, Rate Limiter, Time Limiter) é um módulo independente que pode ser composto conforme necessário.
    \item \textbf{Menor footprint:} Não requer thread pools separados como o Hystrix, reduzindo consumo de recursos.
    \item \textbf{Suporte a programação funcional:} Integração nativa com lambdas Java 8+, CompletableFuture e frameworks reativos.
    \item \textbf{Métricas nativas:} Exportação de métricas para Prometheus/Micrometer sem configuração adicional.
    \item \textbf{Janela deslizante configurável:} Suporta janelas baseadas em contagem ou tempo para cálculo de métricas.
\end{itemize}

\subsection{Trabalhos Relacionados}
A literatura recente apresenta diversos estudos sobre resiliência em microsserviços. Montesi e Weber \cite{montesi2016} analisam a interação entre Circuit Breakers e API Gateways, propondo padrões de composição. Burns \cite{burns2018} contextualiza padrões de resiliência no design de sistemas distribuídos modernos.

De particular relevância para este trabalho é o estudo de Pinheiro, Dantas et al. \cite{pinheiro2024}, que propõe uma modelagem analítica do comportamento de Circuit Breakers utilizando Redes de Petri Estocásticas (SPNs). Esta abordagem permite prever o impacto de diferentes parametrizações do CB em métricas de SLA antes da implantação em produção. O presente TCC complementa essa contribuição teórica ao fornecer \textbf{validação empírica} dos benefícios do Circuit Breaker através de experimentos controlados em um ambiente realista.

A documentação oficial da Microsoft Azure \cite{microsoftpatterns} apresenta o Circuit Breaker como um dos padrões essenciais para aplicações cloud-native, reforçando sua relevância em arquiteturas modernas de larga escala.

\section{A Lacuna e a Justificativa}
Apesar da vasta literatura sobre arquiteturas de microsserviços e padrões de resiliência, observa-se uma lacuna significativa no que diz respeito a \textbf{estudos experimentais quantitativos} que demonstrem, com dados empíricos, o impacto real do padrão Circuit Breaker. Grande parte da documentação disponível limita-se a descrições conceituais ou exemplos triviais que não capturam a complexidade de cenários reais de falha.

Este TCC preenche essa lacuna ao \textbf{implementar} uma POC (Prova de Conceito) que simula um ecossistema de microsserviços com dependência síncrona, instrumentado com Resilience4j, e \textbf{executar} campanhas de benchmark com Docker e k6 para \textbf{medir} empiricamente vazão, latência (p95) e taxa de erro sob cenários controlados. Embora o contexto escolhido seja pagamentos — por familiaridade do autor com o domínio — os padrões de falha simulados (indisponibilidade, degradação, rajadas) são \textbf{universais} em sistemas distribuídos, tornando os resultados aplicáveis a e-commerce, logística, saúde, IoT e qualquer domínio que utilize comunicação síncrona entre serviços.

\section{Objetivos}
\textbf{Objetivo Geral.} Avaliar quantitativamente o impacto do padrão Circuit Breaker no desempenho e na resiliência de microsserviços com comunicação síncrona, utilizando uma POC no domínio de pagamentos como estudo de caso.

\textbf{Objetivos Específicos.}
\begin{enumerate}
    \item Implementar uma POC simplificada composta por \texttt{servico-pagamento} e \texttt{servico-adquirente}, utilizando Spring Boot, Spring Cloud OpenFeign e orquestração via Docker.
    \item Desenvolver duas versões do \texttt{servico-pagamento}: (V1) Baseline com timeouts básicos e (V2) com Resilience4j, Circuit Breaker e fallback.
    \item Construir e executar benchmarks automatizados com k6 para simular cenários de falha universais: indisponibilidade total, degradação gradual, rajadas intermitentes e indisponibilidade prolongada.
    \item Analisar comparativamente as métricas de vazão, latência e taxa de erro, destacando os benefícios e custos da adoção do Circuit Breaker.
\end{enumerate}

\section{Metodologia e Design do Experimento}

\subsection{Visão Geral da Metodologia}
Este Trabalho de Conclusão de Curso adota uma abordagem de \textbf{pesquisa experimental quantitativa}. O método consiste em construir uma \textbf{POC (Prova de Conceito) simplificada} que simula um ecossistema de microsserviços com dependência síncrona. A POC é intencionalmente minimalista — sem banco de dados, cache ou autenticação — para isolar o efeito do Circuit Breaker como única variável de interesse.

A investigação compara duas variações arquiteturais do \texttt{servico-pagamento}: (i) uma versão Baseline, sem mecanismos avançados de resiliência, e (ii) uma versão com Circuit Breaker implementado via Resilience4j. Ambas são orquestradas com \texttt{Docker Compose} e submetidas a campanhas de testes de carga com \texttt{k6}, permitindo coletar métricas objetivas de desempenho (vazão e latência) e resiliência (taxa de erro).

\subsection{Ambiente Experimental}
Os experimentos foram executados em um ambiente controlado com as seguintes especificações:

\textbf{Hardware:}
\begin{itemize}
    \item \textbf{Processador:} Apple M1 Pro (8 núcleos de desempenho + 2 de eficiência)
    \item \textbf{Memória RAM:} 16 GB LPDDR5
    \item \textbf{Armazenamento:} SSD NVMe 512 GB
    \item \textbf{Sistema Operacional:} macOS Sonoma 14.x
\end{itemize}

\textbf{Versões de Software:}
\begin{itemize}
    \item \textbf{Docker Desktop:} 4.25+ com Docker Engine 24.0+
    \item \textbf{Docker Compose:} v2.20+
    \item \textbf{Java:} OpenJDK 17.0.8 (Eclipse Temurin)
    \item \textbf{Spring Boot:} 3.1.5
    \item \textbf{Resilience4j:} 2.1.0
    \item \textbf{Grafana k6:} v0.46.0
\end{itemize}

\textbf{Configuração dos Contêineres Docker:}
\begin{itemize}
    \item \textbf{servico-pagamento:} 1 GB de memória, 1 CPU
    \item \textbf{servico-adquirente:} 512 MB de memória, 0.5 CPU
    \item \textbf{Rede:} Bridge network dedicada para isolamento
\end{itemize}

\textbf{Configuração do Circuit Breaker (V2):}
\begin{itemize}
    \item \texttt{failureRateThreshold}: 50\%
    \item \texttt{slowCallRateThreshold}: 70\%
    \item \texttt{slowCallDurationThreshold}: 3000ms
    \item \texttt{slidingWindowType}: COUNT\_BASED
    \item \texttt{slidingWindowSize}: 10 requisições
    \item \texttt{minimumNumberOfCalls}: 5
    \item \texttt{waitDurationInOpenState}: 10s
    \item \texttt{permittedNumberOfCallsInHalfOpenState}: 3
\end{itemize}

\textbf{Repetições:} Cada cenário foi executado uma única vez por versão (V1 e V2), totalizando 8 execuções. A duração dos testes (9 a 13 minutos por cenário) e o volume de requisições (60.000 a 83.000 por teste) fornecem amostras estatisticamente significativas para análise.

\subsection{Ferramentas e Tecnologias (O Stack)}
O experimento será conduzido com um conjunto integrado de ferramentas que sustentam tanto o desenvolvimento quanto a execução controlada dos cenários de teste:

\begin{figure}[htbp]
\centering
\includegraphics[width=0.8\textwidth]{images/arquitetura_simplificada.png}
\caption{Stack de Monitoramento e Testes}
\label{fig:stack}
\end{figure}

\begin{itemize}
    \item \textbf{Java 17+ e Spring Boot 3:} fundamentos para a implementação dos microsserviços do ecossistema de pagamentos, oferecendo um ambiente moderno, performático e amplamente suportado pela comunidade.
    \item \textbf{Spring Cloud OpenFeign:} cliente declarativo utilizado para a comunicação síncrona entre \texttt{servico-pagamento} e \texttt{servico-adquirente}, simplificando a integração entre serviços através de interfaces anotadas.
    \item \textbf{Spring Cloud Resilience4j:} biblioteca responsável por fornecer o mecanismo de Circuit Breaker e demais padrões de tolerância a falhas, incluindo suporte nativo para métricas, fallbacks e configurações flexíveis.
    \item \textbf{Docker e Docker Compose:} garantem que o ambiente experimental seja isolado, reproduzível e facilmente provisionado, permitindo a execução consistente dos testes em qualquer máquina.
    \item \textbf{Grafana k6:} ferramenta de testes de carga de código aberto escrita em JavaScript/Go, que permite descrever cenários complexos com usuários virtuais (VUs), estágios de carga progressivos e \texttt{thresholds} para validação automática de SLAs. O k6 exporta métricas detalhadas em formato JSON para análise posterior.
\end{itemize}

\subsection{Arquitetura do Sistema Experimental}
O ambiente experimental consiste em uma \textbf{Prova de Conceito (POC) simplificada}, propositalmente rudimentar, desenvolvida para isolar e evidenciar o comportamento do Circuit Breaker sem a complexidade de um sistema de produção real. A simplicidade é intencional: permite atribuir com clareza as diferenças observadas exclusivamente ao padrão de resiliência, sem variáveis de confusão como banco de dados, cache, autenticação ou múltiplas dependências.

A POC é composta por dois microsserviços desenvolvidos em Spring Boot e empacotados como contêineres Docker:

\subsubsection{\texttt{servico-adquirente} — Ponto de Falha Controlado}
\begin{itemize}
    \item \textbf{Função:} simular um gateway externo de pagamento (por exemplo, Cielo ou Rede) responsável por autorizar transações.
    \item \textbf{Endpoint:} expõe \texttt{POST /autorizar} para receber solicitações de autorização.
    \item \textbf{Controle Experimental:} o comportamento é configurável via o parâmetro de query \texttt{?modo=}:
    \begin{itemize}
        \item \texttt{modo=normal}: responde em aproximadamente 50 ms com HTTP 200 (OK). Simula operação saudável.
        \item \texttt{modo=latencia}: responde em 3000 ms (utilizando \texttt{Thread.sleep()}) com HTTP 200 (OK). Simula degradação de performance.
        \item \texttt{modo=falha}: responde imediatamente com HTTP 500 (Internal Server Error). Simula indisponibilidade.
    \end{itemize}
\end{itemize}

\subsubsection{Respostas Possíveis do Sistema}

O \texttt{servico-pagamento} pode retornar diferentes códigos HTTP dependendo do estado do sistema e da versão (V1 ou V2):

\begin{table}[H]
\centering
\caption{Códigos de Resposta HTTP e seus Significados}
\label{tab:http-responses}
\begin{tabular}{clcc}
\toprule
\textbf{Status} & \textbf{Significado} & \textbf{Origem} & \textbf{Versão} \\
\midrule
200/201 & Sucesso real & API funcionou & V1, V2 \\
202 & Fallback (pagamento agendado) & CB aberto & V2 \\
500 & Erro da API externa & Falha propagada & V1, V2 \\
503 & CB rejeitando requisições & CB aberto & V2 \\
\bottomrule
\end{tabular}
\end{table}

O código HTTP 202 (Accepted) é particularmente importante pois representa a \textbf{degradação graciosa}: o sistema informa ao usuário que o pagamento foi recebido e será processado posteriormente, em vez de retornar um erro.

\subsubsection{\texttt{servico-pagamento} — Sistema Sob Teste}
\begin{itemize}
    \item \textbf{Função:} orquestrar o fluxo de pagamento exposto aos clientes finais e consumir o \texttt{servico-adquirente} síncronamente.
    \item \textbf{Endpoint:} expõe \texttt{POST /pagar}, que será exercitado pelos scripts do k6.
    \item \textbf{Lógica de Negócio:} utiliza um Feign Client para delegar a autorização ao \texttt{servico-adquirente}. Este serviço possuirá duas versões, que configuram a variável independente do experimento.
\end{itemize}

\subsection{Variáveis do Experimento}

\subsubsection{Variável Independente — Estratégia de Resiliência no \texttt{servico-pagamento}}
\begin{itemize}
    \item \textbf{V1: Baseline (Controle):} implementação ingênua que apenas configura timeouts curtos (por exemplo, 2 segundos) no Feign Client para conexão e leitura.
    \item \textbf{V2: Circuit Breaker + Fallback:} implementação robusta com Resilience4j. O Circuit Breaker é configurado para abrir após detectar, por exemplo, 50\% de falhas em uma janela de requisições. Quando o circuito está aberto, um método de fallback devolve HTTP 202 (Accepted) com a mensagem "Pagamento recebido e agendado para processamento posterior.", caracterizando a degradação graciosa.
\end{itemize}

\subsubsection{Variáveis Dependentes — Métricas coletadas via k6}
\begin{itemize}
    \item \textbf{\texttt{http\_reqs}:} número total de requisições processadas, utilizado como medida de vazão.
    \item \textbf{\texttt{http\_req\_duration\{p(95)\}}:} percentil 95 do tempo de resposta, indicador de latência sob carga.
    \item \textbf{\texttt{http\_req\_failed}:} taxa de requisições consideradas falhas pelo k6, refletindo a resiliência percebida.
\end{itemize}

\subsection{Plano de Execução — Cenários de Teste k6}
Quatro cenários de teste foram desenvolvidos em k6, cada um projetado para exercitar diferentes aspectos do comportamento do sistema sob estresse. Cada cenário foi executado duas vezes: uma para a versão Baseline (V1) e outra para a versão com Circuit Breaker (V2). Os cenários utilizam de 50 a 200 usuários virtuais (VUs) com duração variável para simular diferentes padrões de carga realistas.

\subsubsection{Cenário 1 — Falha Catastrófica}

\textbf{Contexto Real:} Este cenário reproduz situações como deploy problemático em produção, queda total de servidor, ou falha de infraestrutura de rede. São eventos raros, porém de alto impacto quando ocorrem.

\begin{itemize}
    \item \textbf{Descrição:} A API externa (\texttt{servico-adquirente}) fica \textbf{100\% indisponível} por 5 minutos consecutivos, retornando erro HTTP 500 em todas as requisições.
    
    \item \textbf{Fases do Teste (13 minutos):}
    \begin{enumerate}
        \item \textbf{Aquecimento} (0-1min): Rampa de 0 a 50 VUs, operação normal.
        \item \textbf{Operação Normal} (1-4min): 100 VUs, distribuição realista (70\% sucesso, 20\% latência, 10\% falha).
        \item \textbf{CATÁSTROFE} (4-9min): 150 VUs, \textbf{100\% das requisições em modo falha}. Este é o momento crítico.
        \item \textbf{Recuperação} (9-12min): API volta ao normal gradualmente (60\% sucesso, 25\% latência, 15\% falha).
        \item \textbf{Cooldown} (12-13min): Redução a 0 VUs.
    \end{enumerate}
    
    \item \textbf{Comportamento Esperado:}
    \begin{itemize}
        \item \textit{V1 (Baseline):} Todas as requisições aguardam timeout (~3s), threads ficam bloqueadas, potencial cascata de falhas.
        \item \textit{V2 (Circuit Breaker):} CB detecta falhas em ~10s, abre circuito, retorna fallback (HTTP 202) em <100ms.
    \end{itemize}
    
    \item \textbf{Objetivo:} Verificar se o CB abre rapidamente durante falha total, protege recursos e mantém o sistema responsivo via fallback.
    \item \textbf{Threshold:} \texttt{http\_req\_duration\{p(95)\} < 1000ms}.
\end{itemize}

\subsubsection{Cenário 2 — Degradação Gradual}

\textbf{Contexto Real:} Este cenário reproduz a situação mais comum em produção: um serviço que começa saudável mas degrada progressivamente devido a memory leaks, pool de conexões esgotando, CPU saturando ou acúmulo de requisições em fila.

\begin{itemize}
    \item \textbf{Descrição:} A taxa de falhas e latência da API externa \textbf{aumenta gradualmente} ao longo do teste, simulando degradação progressiva típica de problemas de infraestrutura.
    
    \item \textbf{Fases do Teste (13 minutos):}
    \begin{enumerate}
        \item \textbf{Sistema Saudável} (0-2min): 100 VUs, baixa taxa de erro (5\% falha, 15\% latência, 80\% normal).
        \item \textbf{Degradação Inicial} (2-5min): 150 VUs, degradação perceptível (20\% falha, 30\% latência, 50\% normal).
        \item \textbf{CRÍTICO} (5-8min): 200 VUs, sistema sob estresse severo (50\% falha, 40\% latência, 10\% normal).
        \item \textbf{Recuperação} (8-12min): 100 VUs, melhora gradual (15\% falha, 25\% latência).
        \item \textbf{Cooldown} (12-13min): Encerramento.
    \end{enumerate}
    
    \item \textbf{Comportamento Esperado:}
    \begin{itemize}
        \item \textit{V1 (Baseline):} Degrada junto com a API, usuários experimentam latência crescente.
        \item \textit{V2 (Circuit Breaker):} CB pode detectar degradação e isolar antes do colapso total.
    \end{itemize}
    
    \item \textbf{Objetivo:} Avaliar se o CB detecta degradação precoce e isola o problema antes da cascata. Este cenário também valida que o CB \textbf{não introduz overhead} em condições moderadas.
\end{itemize}

\subsubsection{Cenário 3 — Rajadas Intermitentes}

\textbf{Contexto Real:} Este cenário reproduz instabilidades de rede, problemas intermitentes de DNS, ou serviços que reiniciam frequentemente. É o \textbf{pior cenário para sistemas sem CB}, pois ficam constantemente oscilando entre funcionar e falhar.

\begin{itemize}
    \item \textbf{Descrição:} Períodos de \textbf{100\% falha} (1 minuto) alternados com períodos de operação normal (2 minutos), repetindo 3 vezes.
    
    \item \textbf{Padrão de Rajadas (13 minutos):}
    \begin{enumerate}
        \item \textbf{Aquecimento} (0-1min): 100 VUs, operação normal.
        \item \textbf{Normal} (1-3min): 150 VUs (80\% sucesso, 15\% latência, 5\% falha).
        \item \textbf{RAJADA 1} (3-4min): 200 VUs, \textbf{100\% falha}.
        \item \textbf{Normal} (4-6min): 150 VUs, operação normal.
        \item \textbf{RAJADA 2} (6-7min): 200 VUs, \textbf{100\% falha}.
        \item \textbf{Normal} (7-9min): 150 VUs, operação normal.
        \item \textbf{RAJADA 3} (9-10min): 200 VUs, \textbf{100\% falha}.
        \item \textbf{Normal} (10-12min): 150 VUs, operação normal.
        \item \textbf{Cooldown} (12-13min): Encerramento.
    \end{enumerate}
    
    \item \textbf{Comportamento Esperado:}
    \begin{itemize}
        \item \textit{V1 (Baseline):} Sofre com cada rajada (100\% erro), recupera entre elas.
        \item \textit{V2 (Circuit Breaker):} CB abre nas rajadas (~8s após início), fecha nos períodos normais. Demonstra \textbf{elasticidade}.
    \end{itemize}
    
    \item \textbf{Objetivo:} Testar a capacidade do CB de \textbf{transicionar dinamicamente} entre estados (Fechado $\rightarrow$ Aberto $\rightarrow$ Semiaberto $\rightarrow$ Fechado) conforme o estado da dependência muda.
\end{itemize}

\subsubsection{Cenário 4 — Indisponibilidade Extrema (75\% OFF)}

\textbf{Contexto Real:} Este é o cenário mais extremo, simulando janelas prolongadas de manutenção não programada, falhas de datacenter, ou dependências externas com SLA muito baixo. Demonstra o \textbf{ganho máximo} do Circuit Breaker.

\begin{itemize}
    \item \textbf{Descrição:} A API externa passa \textbf{75\% do tempo indisponível}, incluindo uma janela contínua de 4 minutos de falha total. Este cenário é propositalmente severo para evidenciar o valor do fallback.
    
    \item \textbf{Fases do Teste (9 minutos):}
    \begin{enumerate}
        \item \textbf{Aquecimento} (0-45s): 80 VUs, operação controlada.
        \item \textbf{Operação Saudável} (45s-1.5min): 140 VUs, funcionamento normal.
        \item \textbf{FALHA PROLONGADA} (1.5-5.5min): 180 VUs, \textbf{4 minutos contínuos de indisponibilidade} (100\% falha).
        \item \textbf{Instabilidade} (5.5-7.5min): 200 VUs, rajadas adicionais com alta taxa de falha.
        \item \textbf{Recuperação} (7.5-8.5min): 140 VUs, retorno gradual.
        \item \textbf{Cooldown} (8.5-9min): Encerramento.
    \end{enumerate}
    
    \item \textbf{Padrão de Indisponibilidade:}
    \begin{itemize}
        \item Ciclos de 80 segundos: 60s em falha + 20s de recuperação parcial.
        \item Janela crítica central: 4 minutos de falha \textbf{ininterrupta}.
    \end{itemize}
    
    \item \textbf{Comportamento Esperado:}
    \begin{itemize}
        \item \textit{V1 (Baseline):} Sistema praticamente \textbf{inutilizável} (~10\% sucesso). Caracteriza falha catastrófica.
        \item \textit{V2 (Circuit Breaker):} CB mantém circuito aberto, serve fallbacks continuamente, preservando \textbf{~97\% de disponibilidade percebida}.
    \end{itemize}
    
    \item \textbf{Objetivo:} Validar que o CB transforma um sistema inutilizável em um sistema funcional através da degradação graciosa.
    \item \textbf{Thresholds:} \texttt{http\_req\_duration\{p(95)\} < 1200ms} e \texttt{http\_req\_failed < 0.25}.
\end{itemize}

\subsubsection{Resumo Comparativo dos Cenários}

A Tabela \ref{tab:cenarios-resumo} apresenta uma visão consolidada dos quatro cenários, destacando suas características distintivas e o propósito de cada um no experimento.

\begin{table}[H]
\centering
\caption{Características dos Cenários de Teste}
\label{tab:cenarios-resumo}
\begin{tabular}{lcccl}
\toprule
\textbf{Cenário} & \textbf{Duração} & \textbf{VUs} & \textbf{Padrão de Falha} & \textbf{Testa} \\
\midrule
Catastrófica & 13min & 50-150 & 100\% falha por 5min & Fail-fast \\
Degradação & 13min & 100-200 & 5\% → 50\% gradual & Detecção precoce \\
Rajadas & 13min & 100-200 & 3× (100\% por 1min) & Elasticidade \\
Indisponibilidade & 9min & 80-200 & 75\% offline & Ganho máximo \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Procedimento de Análise Estatística}

Para validar estatisticamente as diferenças observadas entre as versões V1 (Baseline) e V2 (Circuit Breaker), foi definido um procedimento de análise estatística robusto, considerando as características dos dados coletados.

\subsubsection{Justificativa para Testes Não-Paramétricos}

A escolha por testes não-paramétricos foi motivada pelas seguintes características dos dados:

\begin{itemize}
    \item \textbf{Distribuição não-normal:} Tempos de resposta em sistemas distribuídos tipicamente apresentam distribuição assimétrica com cauda longa, violando a premissa de normalidade exigida por testes paramétricos como o teste t de Student.
    \item \textbf{Grande volume amostral:} Com mais de 380.000 requisições por versão, testes de normalidade (Shapiro-Wilk, Kolmogorov-Smirnov) tendem a rejeitar a hipótese nula mesmo para desvios mínimos da normalidade.
    \item \textbf{Presença de outliers:} Timeouts e respostas de fallback introduzem valores extremos que podem distorcer estimativas baseadas em média e desvio padrão.
\end{itemize}

\subsubsection{Testes Estatísticos Selecionados}

\textbf{Teste de Mann-Whitney U:} Teste não-paramétrico para comparação de duas amostras independentes, utilizado para verificar se as distribuições de tempos de resposta de V1 e V2 são estatisticamente diferentes. O nível de significância adotado foi $\alpha = 0,05$.

\textbf{Teste de Kolmogorov-Smirnov (KS):} Complementarmente, o teste KS foi aplicado para avaliar se as duas distribuições diferem em forma, não apenas em tendência central.

\subsubsection{Medida de Tamanho do Efeito}

\textbf{Cliff's Delta ($\delta$):} Para quantificar a magnitude prática da diferença entre V1 e V2, foi calculado o Cliff's Delta, uma medida de tamanho de efeito não-paramétrica. A interpretação segue a convenção de \cite{romano2006}:

\begin{itemize}
    \item $|\delta| < 0,147$: efeito \textbf{negligível}
    \item $0,147 \leq |\delta| < 0,33$: efeito \textbf{pequeno}
    \item $0,33 \leq |\delta| < 0,474$: efeito \textbf{médio}
    \item $|\delta| \geq 0,474$: efeito \textbf{grande}
\end{itemize}

Esta medida é particularmente importante porque, com amostras muito grandes, diferenças estatisticamente significativas (p $<$ 0,05) podem não ter relevância prática. O Cliff's Delta permite distinguir entre significância estatística e significância prática.

\subsubsection{Intervalo de Confiança}

Para estimar a diferença média entre as versões, foi calculado um intervalo de confiança de 95\% utilizando bootstrap (1.000 reamostragens), técnica adequada para distribuições não-normais.

\section{Resultados e Discussão}

\subsection{Introdução ao Capítulo}
Os testes de carga foram executados usando \texttt{k6} e \texttt{Docker Compose}. Cada versão do \texttt{servico-pagamento} (V1-Baseline, V2-CircuitBreaker) foi submetida aos quatro cenários de estresse (Falha Catastrófica, Degradação Gradual, Rajadas Intermitentes e Indisponibilidade Extrema). Os resultados foram avaliados contra os \texttt{thresholds} (limites de desempenho) definidos nos scripts e analisados em termos de taxa de sucesso, tempo de resposta e contribuição do mecanismo de fallback.

\subsection{Visão Geral Consolidada dos Resultados}

\begin{table}[H]
\centering
\caption{Resumo Consolidado: Comparação V1 vs V2 em Todos os Cenários}
\begin{tabular}{lcccccc}
\toprule
Cenário & V1 Sucesso & V2 Sucesso & V2 Fallback & Ganho & Red. Falhas \\
\midrule
Catastrófica & 90.02\% & 94.49\% & 59.00\% & +4.47pp & -44.82\% \\
Degradação & 94.72\% & 94.94\% & 0.00\% & +0.22pp & -4.23\% \\
Indisponibilidade & 10.14\% & 97.08\% & 92.80\% & +86.94pp & -96.77\% \\
Rajadas & 94.93\% & 95.21\% & 10.15\% & +0.28pp & -5.76\% \\
\bottomrule
\end{tabular}
\end{table}

A Tabela 1 apresenta o resumo consolidado dos quatro cenários. O dado mais impressionante é o cenário de \textbf{Indisponibilidade Extrema}, onde a V1 apresentou apenas 10.14\% de sucesso, enquanto a V2 alcançou 97.08\% \textemdash\ um ganho de \textbf{86.94 pontos percentuais} e redução de 96.77\% nas falhas. Este resultado demonstra inequivocamente o valor do Circuit Breaker em cenários de alta indisponibilidade.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/01_success_rates_comparison.png}
\caption{Comparação de Taxas de Sucesso: V1 (Baseline) vs V2 (Circuit Breaker)}
\label{fig:success_rates}
\end{figure}

\subsection{Análise do Cenário 1: Falha Catastrófica}

Neste cenário, a API externa ficou 100\% indisponível durante 5 minutos consecutivos, simulando uma queda total do servidor.

\textbf{Comportamento Observado:}
\begin{itemize}
    \item \textbf{V1 (Baseline):} Durante o período de catástrofe, todas as requisições aguardaram o timeout (2s) antes de falhar, consumindo threads e degradando o sistema. A taxa de sucesso geral foi de 90.02\%.
    \item \textbf{V2 (Circuit Breaker):} O CB detectou as falhas em menos de 10 segundos, abriu o circuito e passou a retornar respostas via fallback (HTTP 202) em menos de 100ms. A taxa de sucesso subiu para 94.49\%, com 59\% das respostas sendo fallbacks.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/07_catastrofe_timeline.png}
\caption{Timeline do Cenário de Falha Catastrófica: Transição de Estados}
\label{fig:catastrofe_timeline}
\end{figure}

\textbf{Impacto Positivo do Circuit Breaker:}
\begin{enumerate}
    \item \textbf{Proteção contra Exaustão de Recursos:} Enquanto a V1 mantinha threads bloqueadas aguardando timeout, a V2 liberava threads imediatamente após a abertura do circuito.
    \item \textbf{Tempo de Resposta Previsível:} Média de resposta da V2 durante catástrofe: ~85ms (fallback). V1: ~2000ms (timeout).
    \item \textbf{Degradação Graciosa:} Usuários receberam HTTP 202 informando que o pagamento foi agendado, em vez de erro HTTP 500.
\end{enumerate}

\subsection{Análise do Cenário 2: Degradação Gradual}

Este cenário simula uma situação comum em produção: um serviço que começa saudável mas degrada progressivamente.

\textbf{Comportamento Observado:}
\begin{itemize}
    \item \textbf{V1 (Baseline):} A degradação afetou uniformemente todas as requisições. Taxa de sucesso: 94.72\%.
    \item \textbf{V2 (Circuit Breaker):} O CB detectou o aumento na taxa de falhas e latência, mantendo o sistema estável. Taxa de sucesso: 94.94\%, com fallback praticamente não acionado (0.0\%).
\end{itemize}

\textbf{Insight:} Neste cenário, o ganho foi marginal (+0.22pp) porque a degradação não foi severa o suficiente para acionar o CB consistentemente. Isso demonstra que o CB \textbf{não introduz overhead} em cenários de degradação moderada.

\subsection{Análise do Cenário 3: Rajadas Intermitentes}

O cenário de rajadas é particularmente desafiador, pois exige que o sistema reaja rapidamente a mudanças de estado.

\textbf{Comportamento Observado:}
\begin{itemize}
    \item \textbf{V1 (Baseline):} Durante cada rajada de 1 minuto, 100\% das requisições falharam. Nos períodos normais, o sistema se recuperou. Taxa geral: 94.93\%.
    \item \textbf{V2 (Circuit Breaker):} O CB abriu durante as rajadas e fechou nos períodos normais, demonstrando \textbf{elasticidade}. Taxa geral: 95.21\%, com 10.15\% de fallbacks.
\end{itemize}

\textbf{Elasticidade do Circuit Breaker:} A capacidade do CB de transicionar entre estados (Fechado $\rightarrow$ Aberto $\rightarrow$ Semiaberto $\rightarrow$ Fechado) foi validada neste cenário. O tempo médio para abertura do circuito foi de ~8 segundos após o início de cada rajada.

\subsection{Análise do Cenário 4: Indisponibilidade Extrema (75\% OFF)}

Este é o cenário mais extremo e onde o Circuit Breaker demonstrou seu valor máximo.

\begin{table}[H]
\centering
\caption{Resultados Detalhados: Indisponibilidade Extrema}
\begin{tabular}{lcc}
\toprule
Métrica & V1 (Baseline) & V2 (Circuit Breaker) \\
\midrule
Total de Requisições & 69.252 & 76.967 \\
Taxa de Sucesso Total & 10.14\% & 97.08\% \\
Requisições via Fallback & N/A & 92.80\% \\
Taxa de Falha Real & 89.86\% & 2.91\% \\
Redução de Falhas & \textemdash & \textbf{96.77\%} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Comportamento Observado:}
\begin{itemize}
    \item \textbf{V1 (Baseline):} Com 75\% de indisponibilidade, o sistema se tornou praticamente inutilizável. Apenas 10.14\% das requisições foram bem-sucedidas, caracterizando \textbf{falha catastrófica do serviço}.
    \item \textbf{V2 (Circuit Breaker):} O CB manteve o circuito aberto durante os períodos de indisponibilidade, servindo fallbacks e preservando a disponibilidade percebida pelo usuário em 97.08\%.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/02_failure_reduction.png}
\caption{Redução de Falhas: Impacto do Circuit Breaker por Cenário}
\label{fig:failure_reduction}
\end{figure}

\subsection{Análise de Tempos de Resposta}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/03_response_time_percentiles.png}
\caption{Distribuição de Tempos de Resposta (Percentis) por Cenário}
\label{fig:response_percentiles}
\end{figure}

A análise dos percentis de tempo de resposta revela outro benefício crucial do Circuit Breaker: \textbf{previsibilidade}. Enquanto a V1 apresentou alta variância nos tempos de resposta (de 50ms a 2000ms dependendo do estado da dependência), a V2 manteve tempos consistentemente baixos devido ao mecanismo de fail-fast.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/distributions.png}
\caption{Distribuição Estatística dos Tempos de Resposta: V1 vs V2}
\label{fig:distributions}
\end{figure}

A Figura \ref{fig:distributions} apresenta a distribuição estatística dos tempos de resposta para ambas as versões. Note a concentração de requisições em tempos baixos (mediana ~4ms) com uma cauda longa até os timeouts (~3000ms), característica comum em sistemas com dependências degradadas.

\subsection{Contribuição do Mecanismo de Fallback}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/08_fallback_contribution.png}
\caption{Contribuição do Fallback para a Taxa de Sucesso da V2}
\label{fig:fallback_contribution}
\end{figure}

O gráfico da Figura \ref{fig:fallback_contribution} ilustra como o fallback contribuiu para a taxa de sucesso em cada cenário. No cenário de Indisponibilidade Extrema, \textbf{92.80\%} das respostas bem-sucedidas vieram do fallback, demonstrando que o sistema manteve sua utilidade mesmo com a dependência quase completamente indisponível.

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/timeline_comparison.png}
\caption{Comparação Temporal de Tempos de Resposta: V1 vs V2}
\label{fig:timeline_comparison}
\end{figure}

A Figura \ref{fig:timeline_comparison} apresenta a evolução temporal dos tempos de resposta ao longo de todos os cenários de teste. Observa-se que ambas as versões apresentam picos de latência nos mesmos momentos (correspondentes às fases de estresse), porém a V2 demonstra recuperação mais rápida devido ao mecanismo de fallback.

\subsection{Análise de Throughput}

\begin{figure}[H]
\centering
\includegraphics[width=0.85\textwidth]{images/04_throughput_comparison.png}
\caption{Comparação de Throughput (Requisições/segundo) entre V1 e V2}
\label{fig:throughput}
\end{figure}

Além da taxa de sucesso, o throughput (vazão) é uma métrica crítica. A V2 manteve throughput superior em cenários de falha porque não desperdiçou threads aguardando timeouts. Isso se traduz em maior capacidade de processamento sob estresse.

\subsection{Análise Estatística Formal}

Para validar estatisticamente a diferença entre as versões V1 e V2, aplicamos testes não-paramétricos dado o grande volume de dados (N > 380.000 requisições por versão) e a distribuição não-normal dos tempos de resposta.

\begin{table}[H]
\centering
\caption{Análise Estatística Comparativa: Tempos de Resposta V1 vs V2}
\label{tab:analise-estatistica}
\begin{tabular}{lr}
\toprule
Métrica & Valor \\
\midrule
N (V1) & 381.549 \\
N (V2) & 383.071 \\
Média (V1) & 612,23 ms \\
Média (V2) & 605,94 ms \\
Mediana (V1) & 4,38 ms \\
Mediana (V2) & 4,26 ms \\
Desvio Padrão (V1) & 1.201,49 ms \\
Desvio Padrão (V2) & 1.197,94 ms \\
P95 (V1) & 3.008,34 ms \\
P95 (V2) & 3.008,00 ms \\
P99 (V1) & 3.036,34 ms \\
P99 (V2) & 3.030,40 ms \\
\midrule
Mann-Whitney U & 74.211.797.463,50 \\
p-valor (Mann-Whitney) & 9,37e-32 \\
Kolmogorov-Smirnov D & 0,0160 \\
p-valor (KS) & 5,37e-43 \\
Cliff's Delta ($\delta$) & 0,0155 \\
Interpretação Effect Size & Negligível \\
IC 95\% Diferença & [1,07; 11,77] ms \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Interpretação dos Resultados Estatísticos:}

O teste de Mann-Whitney U revela uma diferença estatisticamente significativa entre V1 e V2 (p < 0,001), indicando que as distribuições de tempos de resposta são distintas. Contudo, o \textbf{Cliff's Delta} ($\delta = 0,0155$) classifica o \textit{effect size} como \textbf{negligível}, sugerindo que, embora a diferença seja estatisticamente detectável devido ao grande volume amostral, sua magnitude prática é mínima.

A diferença média estimada entre V1 e V2 é de aproximadamente 6,29 ms (IC 95\%: 1,07 a 11,77 ms), o que representa uma melhoria de apenas 1,03\% no tempo médio de resposta. Este resultado confirma que o Circuit Breaker \textbf{não introduz overhead perceptível} em operação normal, mantendo desempenho equivalente à versão Baseline enquanto proporciona benefícios substanciais em cenários de falha.

\subsection{Discussão Geral e Impacto do Circuit Breaker}

Os resultados experimentais validam inequivocamente a eficácia do padrão Circuit Breaker. Os principais benefícios observados foram:

\begin{enumerate}
    \item \textbf{Prevenção de Falhas em Cascata:} O CB isolou o \texttt{servico-pagamento} das falhas do \texttt{servico-adquirente}, impedindo que a indisponibilidade se propagasse.
    \item \textbf{Degradação Graciosa:} Em vez de retornar erros HTTP 500, o sistema retornou HTTP 202 com uma mensagem útil ao usuário.
    \item \textbf{Proteção de Recursos:} O mecanismo fail-fast liberou threads rapidamente, evitando thread pool starvation.
    \item \textbf{Overhead Zero em Cenários Normais:} A V2 apresentou desempenho praticamente idêntico à V1 em operação normal.
    \item \textbf{Elasticidade:} O CB demonstrou capacidade de abrir e fechar dinamicamente conforme o estado da dependência.
\end{enumerate}

\section{Conclusão}

\subsection{Revisão dos Objetivos e do Problema}
Este trabalho se propôs a investigar a fragilidade da comunicação síncrona em microsserviços, especificamente o risco de falhas em cascata em um sistema de pagamentos. O objetivo foi avaliar quantitativamente o impacto do padrão Circuit Breaker no desempenho e resiliência, usando um experimento prático e reprodutível com \texttt{Docker} e \texttt{k6}.

\subsection{Síntese dos Resultados}
Os resultados experimentais foram conclusivos e demonstraram de forma inequívoca o valor do padrão Circuit Breaker:

\begin{itemize}
    \item \textbf{Cenário de Indisponibilidade Extrema:} A V2 (Circuit Breaker) alcançou 97.08\% de sucesso contra apenas 10.14\% da V1 — um ganho de 86.94 pontos percentuais e redução de 96.77\% nas falhas.
    \item \textbf{Cenário de Falha Catastrófica:} A V2 manteve 94.49\% de sucesso com 59\% das respostas via fallback, enquanto a V1 apresentou falhas massivas durante o período de indisponibilidade.
    \item \textbf{Overhead Zero:} Em cenários de operação normal e degradação moderada, a V2 apresentou desempenho praticamente idêntico à V1, validando que o CB não introduz custos em ``céu azul''.
    \item \textbf{Análise Estatística:} O teste de Mann-Whitney confirmou diferença significativa (p < 0.001), porém o Cliff's Delta ($\delta = 0.0155$) indica effect size negligível, evidenciando que a diferença prática é mínima (~6ms ou 1.03\%).
    \item \textbf{Elasticidade:} O CB demonstrou capacidade de transicionar dinamicamente entre estados conforme o comportamento da dependência.
\end{itemize}

A arquitetura Baseline (V1) provou ser \textbf{inviável para produção} em sistemas de missão crítica, enquanto a V2 (Circuit Breaker) demonstrou robustez absoluta, protegendo o \texttt{servico-pagamento}, mantendo vazão estável e garantindo disponibilidade percebida pelo usuário através da degradação graciosa (HTTP 202).

\subsection{Contribuições do Trabalho}
Este TCC contribui para a literatura ao fornecer:
\begin{enumerate}
    \item \textbf{Evidência Empírica:} Dados quantitativos que demonstram o impacto real do Circuit Breaker em cenários realistas de falha.
    \item \textbf{Metodologia Reprodutível:} Um framework de testes com Docker e k6 que pode ser replicado para avaliar outros padrões de resiliência.
    \item \textbf{Análise Multidimensional:} Avaliação não apenas de taxa de sucesso, mas também de latência, throughput e comportamento temporal do sistema.
\end{enumerate}

\subsection{Limitações do Estudo}
Apesar dos resultados expressivos, este trabalho apresenta limitações que devem ser consideradas na interpretação dos achados:

\begin{enumerate}
    \item \textbf{POC Simplificada:} O sistema experimental é uma Prova de Conceito intencionalmente rudimentar, sem banco de dados, cache, autenticação ou lógica de negócio complexa. Sistemas de produção reais possuem mais variáveis que podem interagir com o comportamento do Circuit Breaker.
    
    \item \textbf{Ambiente Local:} Os experimentos foram executados em uma única máquina, sem latência de rede real entre serviços. Em ambientes distribuídos (multi-datacenter, cloud), a latência adicional pode influenciar os resultados.
    
    \item \textbf{Carga Sintética:} O k6 gera tráfego sintético com padrões uniformes. Tráfego real apresenta características mais complexas: rajadas imprevisíveis, sazonalidade e correlação entre requisições.
    
    \item \textbf{Serviço Adquirente Mockado:} O \texttt{servico-adquirente} foi implementado com falhas controladas e determinísticas. Em produção, falhas são frequentemente parciais e imprevisíveis.
    
    \item \textbf{Execução Única:} Cada cenário foi executado uma vez por versão. Múltiplas execuções permitiriam calcular intervalos de confiança e validar reprodutibilidade.
    
    \item \textbf{Configuração Fixa do CB:} Apenas uma configuração do Circuit Breaker foi testada. Diferentes parametrizações podem resultar em comportamentos distintos.
    
    \item \textbf{Domínio Específico:} Embora o contexto de pagamentos seja generalizável, outros domínios podem ter requisitos específicos (ex: latência ultra-baixa em trading) que influenciam a aplicabilidade dos resultados.
\end{enumerate}

\subsection{Ameaças à Validade}
Identificamos as seguintes ameaças à validade dos resultados:

\textbf{Validade Interna:}
\begin{itemize}
    \item \textbf{Variabilidade do ambiente:} Processos em segundo plano no sistema operacional, garbage collection da JVM e contenção de recursos do Docker podem introduzir variância nos resultados.
    \item \textbf{Warm-up da JVM:} A compilação JIT (Just-In-Time) pode afetar os primeiros minutos de cada teste. Mitigamos isso com fases de aquecimento nos scripts k6.
    \item \textbf{Ordem de execução:} Os testes foram executados sequencialmente; efeitos de ordem (ex: fragmentação de memória) não foram controlados.
\end{itemize}

\textbf{Validade Externa:}
\begin{itemize}
    \item \textbf{Generalização:} Os resultados são específicos para o domínio de pagamentos e a stack tecnológica utilizada (Java/Spring). Outros domínios e tecnologias podem apresentar comportamentos diferentes.
    \item \textbf{Escala:} O experimento utilizou até 200 VUs (usuários virtuais). Sistemas de produção podem enfrentar milhares de requisições simultâneas, alterando a dinâmica de contenção.
    \item \textbf{Complexidade arquitetural:} O experimento envolveu apenas dois serviços. Arquiteturas reais com dezenas de microsserviços introduzem cadeias de dependência mais complexas.
\end{itemize}

\textbf{Validade de Construção:}
\begin{itemize}
    \item \textbf{Definição de sucesso:} Consideramos HTTP 200 e HTTP 202 (fallback) como sucesso. Em contextos onde o fallback não é aceitável pelo negócio, a interpretação dos resultados seria diferente.
    \item \textbf{Métricas selecionadas:} Focamos em taxa de sucesso, latência e throughput. Outras métricas (uso de CPU, memória, conexões abertas) poderiam revelar aspectos adicionais.
\end{itemize}

\subsection{Trabalhos Futuros}
Como trabalho futuro, sugere-se:
\begin{enumerate}
    \item \textbf{Comparação com Outros Padrões:} Expandir o experimento para comparar o Circuit Breaker com outros padrões de resiliência como Retry (com backoff exponencial), Bulkhead (isolamento de threads) e Rate Limiter, avaliando tanto o uso isolado quanto a composição destes padrões.
    
    \item \textbf{Análise Paramétrica:} Investigar sistematicamente o impacto de diferentes configurações do Circuit Breaker (ex: \texttt{slidingWindowSize}, \texttt{failureRateThreshold}, \texttt{waitDurationInOpenState}), identificando configurações ótimas para diferentes perfis de carga.
    
    \item \textbf{Cenários de Múltiplas Dependências:} Avaliar o comportamento do CB em arquiteturas com múltiplos serviços dependentes, investigando estratégias de Circuit Breaker por dependência versus Circuit Breaker global.
    
    \item \textbf{Comunicação Assíncrona:} Comparar os resultados com arquiteturas baseadas em mensageria (Apache Kafka, RabbitMQ), avaliando os trade-offs entre comunicação síncrona protegida por CB e comunicação assíncrona nativa.
    
    \item \textbf{Ambiente Cloud Distribuído:} Replicar os experimentos em ambiente cloud (AWS, GCP ou Azure) com serviços distribuídos geograficamente, introduzindo latência de rede real e avaliando o comportamento do CB em cenários de particionamento de rede.
    
    \item \textbf{Chaos Engineering:} Integrar ferramentas de Chaos Engineering (ex: Chaos Monkey, Litmus) para injeção de falhas aleatórias e contínuas, validando a robustez do Circuit Breaker em condições mais realistas e imprevisíveis.
    
    \item \textbf{Observabilidade Avançada:} Implementar distributed tracing (Jaeger, Zipkin) para correlacionar o estado do Circuit Breaker com traces de requisições, facilitando a análise de causa raiz em cenários complexos.
    
    \item \textbf{Machine Learning para Tuning:} Explorar o uso de algoritmos de aprendizado de máquina para ajuste dinâmico dos parâmetros do Circuit Breaker com base em padrões históricos de tráfego e falhas.
    
    \item \textbf{Estudo Longitudinal:} Conduzir um estudo de longo prazo em ambiente de produção para avaliar a eficácia do Circuit Breaker ao longo de meses, capturando eventos reais de degradação e falha.
\end{enumerate}

\begin{thebibliography}{20}

\bibitem{pinheiro2024}
Pinheiro, B., Dantas, J., et al.: Performance Modeling of Microservices with Circuit Breakers using Stochastic Petri Nets. In: International Conference on Systems and Informatics (2024)

\bibitem{nygard2018}
Nygard, M.T.: Release It! Design and Deploy Production-Ready Software. 2nd edn. Pragmatic Bookshelf (2018)

\bibitem{newman2021}
Newman, S.: Building Microservices: Designing Fine-Grained Systems. 2nd edn. O'Reilly Media (2021)

\bibitem{fowler2014}
Fowler, M., Lewis, J.: Microservices: A Definition of This New Architectural Term. martinfowler.com (2014). \url{https://martinfowler.com/articles/microservices.html}

\bibitem{fowler2014cb}
Fowler, M.: CircuitBreaker. martinfowler.com (2014). \url{https://martinfowler.com/bliki/CircuitBreaker.html}

\bibitem{resilience4j}
Resilience4j: Fault Tolerance Library for Java. GitHub (2024). \url{https://resilience4j.readme.io/docs}

\bibitem{resilience4j-cb}
Resilience4j: CircuitBreaker Documentation. \url{https://resilience4j.readme.io/docs/circuitbreaker}

\bibitem{springcloud}
Spring Cloud: Spring Cloud Circuit Breaker. \url{https://spring.io/projects/spring-cloud-circuitbreaker}

\bibitem{k6docs}
Grafana k6: Load Testing Documentation. \url{https://k6.io/docs/}

\bibitem{k6thresholds}
Grafana k6: Thresholds Documentation. \url{https://k6.io/docs/using-k6/thresholds/}

\bibitem{docker}
Docker: Enterprise Container Platform. \url{https://docs.docker.com/}

\bibitem{springboot}
Spring Boot: Reference Documentation. \url{https://docs.spring.io/spring-boot/docs/current/reference/html/}

\bibitem{feign}
Spring Cloud OpenFeign: Declarative REST Client. \url{https://docs.spring.io/spring-cloud-openfeign/docs/current/reference/html/}

\bibitem{richardson2018}
Richardson, C.: Microservices Patterns: With Examples in Java. Manning Publications (2018)

\bibitem{netflix2016}
Netflix Technology Blog: Making the Netflix API More Resilient. Medium (2016). \url{https://netflixtechblog.com/making-the-netflix-api-more-resilient-a8ec62ddf01b}

\bibitem{hystrix}
Netflix: Hystrix - Latency and Fault Tolerance for Distributed Systems. GitHub. \url{https://github.com/Netflix/Hystrix}

\bibitem{microsoftpatterns}
Microsoft Azure: Cloud Design Patterns - Circuit Breaker. \url{https://docs.microsoft.com/en-us/azure/architecture/patterns/circuit-breaker}

\bibitem{montesi2016}
Montesi, F., Weber, J.: Circuit Breakers, Discovery, and API Gateways in Microservices. arXiv preprint arXiv:1609.05830 (2016)

\bibitem{burns2018}
Burns, B.: Designing Distributed Systems: Patterns and Paradigms for Scalable, Reliable Services. O'Reilly Media (2018)

\bibitem{romano2006}
Romano, J., Kromrey, J.D., Coraggio, J., Skowronek, J.: Appropriate statistics for ordinal level data: Should we really be using t-test and Cohen's d for evaluating group differences on the NSSE and other surveys? In: Annual Meeting of the Florida Association of Institutional Research, pp. 1--33 (2006)

\end{thebibliography}

\end{document}
